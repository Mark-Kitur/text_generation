{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75a04733",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    'epochs': 50,\n",
    "    'batch_size':32,\n",
    "    'lr':0.001,\n",
    "    'd_model':512,\n",
    "    'num_heads':8,\n",
    "    'd_ff': 2048,\n",
    "    'num_layers':6,\n",
    "    'vocab_size': 50000,\n",
    "    'checkpoint_path':\"model.pth\",\n",
    "    'dropout':0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33340a6",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2505618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text,vocab):\n",
    "    words = re.findall(r'\\b\\w+\\b',text.lower())\n",
    "    return [vocab.get(word,vocab[\"UNK\"]) for word in words]\n",
    "\n",
    "def detokenize(tokens,vocab):\n",
    "    reverse_vocab = {v:k for k,v in vocab.items()}\n",
    "    return \" \".join([reverse_vocab.get(token, \"UNK\") for token in tokens])\n",
    "\n",
    "def build_vocab(text, vocab_size=50000):\n",
    "    words = re.findall(r'\\b\\w+\\b',text.lower())\n",
    "    word_counts =Counter(words)\n",
    "    most_common = word_counts.most_common(vocab_size-2)\n",
    "\n",
    "    vocab ={\"<PAD>\":0, \"UNK\":1}\n",
    "    vocab.update({word: idx +2 for idx,(word,_) in enumerate(most_common)})\n",
    "\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a85dc5",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066c4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "\n",
    "def preprocess_text(input_file, output_file, vocab_size:int=5000, seq_len:int=64,encoding:str='utf-8'):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding=encoding) as file:\n",
    "            text= file.read().lower()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(input_file, 'r', encoding='latin-1') as file:\n",
    "            text =file.read().lower()\n",
    "    except Exception as e:\n",
    "        print(f'Error reading file: {e}')\n",
    "    \n",
    "    vocab = build_vocab(text,vocab_size)\n",
    "    tokenize_text = tokenize(text,vocab)\n",
    "\n",
    "    data = [tokenize_text[i:i+seq_len] for i in range(0,len(tokenize_text)-seq_len,seq_len)]\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    torch.save((data, vocab), output_file)\n",
    "    print(f'Data saved to {output_file}')\n",
    "    print(f'Vocabulary size: {len(vocab)}')\n",
    "    print(f'Total token:{len(tokenize_text)}')\n",
    "    print(f'Number of seq:{len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78aaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa244500",
   "metadata": {},
   "source": [
    "# Model Architacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41c8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model, num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert d_model % num_heads ==0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # initialize dimensions\n",
    "        self.d_model = d_model\n",
    "        self.num_heads= num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # linear layers for transforming the inputs queries, keys and valuea\n",
    "        self.W_q = nn.Linear(d_model,d_model)\n",
    "        self.W_k = nn.Linear(d_model,d_model)\n",
    "        self.W_v = nn.Linear(d_model,d_model)\n",
    "        self.W_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def scaled_dot_product_aatention(self,q,k,v,mask=None):\n",
    "        attn_scores =torch.matmul(q,k.transpose(-2,-1))/ math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores =attn_scores.masked_fill(mask==0,float('-inf'))\n",
    "        attn_probs = torch.softmax(attn_scores,dim=-1)\n",
    "\n",
    "        output = torch.matmul(attn_scores,v)\n",
    "        return output\n",
    "    def split_head(self,x):\n",
    "        # reshape the input tensor to (batch_size, seq_len, num_heads, d_k)\n",
    "        batch_size, seq_len, d_model =x.size()\n",
    "        return x.view(batch_size,seq_len, self.num_heads,self.d_k).transpose(1,2)\n",
    "    \n",
    "    def combine_heads(self,x):\n",
    "        batch_size, num_heads, seq_len, d_k =x.size()\n",
    "        return x.transpose(1,2).contiguous().view()(batch_size,seq_len,self.d_model)\n",
    "    \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        # linear transformation and split\n",
    "        Q= self.split_head(self.W_q(q))\n",
    "        K = self.split_head(self.W_k(k))\n",
    "        V = self.split_head(self.W_v(v))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_aatention(Q,K,V,mask)\n",
    "        attn_output =self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0620aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super(PositionWiseFeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        self.activation =nn.GeLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8978d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_len):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,d_model)\n",
    "        position =torch.arange(0, max_seq_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float()* - (math.log(10000.0)/d_model))\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414516de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads,d_ff,dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 =nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        attn_output = self.self_attn(x,x,x,mask)\n",
    "        x =self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x+self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e933b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size, d_model, num_heads, num_layers, d_ff,max_seq_len, dropout,device):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.positional_encoding= PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc= nn.Linear(d_model,vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def genarate_mask (self,data):\n",
    "        # padding mask\n",
    "        pad_mask = (data !=0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # casual mask (seq_len, seq_len)\n",
    "        seq_len = data.size(1)\n",
    "        casual_mask = torch.tril(torch.ones((seq_len,seq_len),device=self.device)).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        # combine all only non-pad + past tokens\n",
    "        mask = pad_mask & casual_mask\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, data):\n",
    "        mask =self.genarate_mask(data)\n",
    "\n",
    "        data_embed = self.dropout(self.positional_encoding(self.decoder_embedding(data)))\n",
    "        x = data_embed\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x,mask)\n",
    "        output = self.fc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd8046",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff2f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config['vocab_size'],\n",
    "                    config['d_model'],\n",
    "                    config['num_heads'],\n",
    "                    config['num_layers'],\n",
    "                    config['d_ff'],\n",
    "                    config['vocab_size'],\n",
    "                    config['dropout'],\n",
    "                    device=device\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(data_path, vocab_size, config, device, TransformerClass):\n",
    "    # Load preprocessed data\n",
    "    data, vocab = torch.load(data_path)   # assume data is (input_seq, target_seq) tensors\n",
    "\n",
    "    # Wrap data in TensorDataset + DataLoader\n",
    "    dataset = TensorDataset(data['input'], data['target'])\n",
    "    data_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = TransformerClass(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        max_seq_len=config['max_seq_len'],\n",
    "        dropout=config['dropout'],\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "\n",
    "    # Create checkpoint folder\n",
    "    os.makedirs(os.path.dirname(config['checkpoint_path']), exist_ok=True)\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_seq, target_seq in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_seq)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "            # Flatten for CE Loss\n",
    "            loss = criterion(\n",
    "                logits.view(-1, vocab_size),\n",
    "                target_seq.view(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * input_seq.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), config['checkpoint_path'])\n",
    "\n",
    "    return model, vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
