{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a04733",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    'epochs': 50,\n",
    "    'batch_size':32,\n",
    "    'lr':0.001,\n",
    "    'd_model':512,\n",
    "    'num_heads':8,\n",
    "    'd_ff': 2048,\n",
    "    'num_layers':6,\n",
    "    'vocab_size': 50000,\n",
    "    'checkpoint_path':\"model.pth\",\n",
    "    'dropout':0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33340a6",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2505618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text,vocab):\n",
    "    words = re.findall(r'\\b\\w+\\b',text.lower())\n",
    "    words.append(\"<EOS>\")\n",
    "    words.insert(0,\"<SOS>\")\n",
    "    return [vocab.get(word,vocab[\"UNK\"]) for word in words]\n",
    "\n",
    "def detokenize(tokens,vocab):\n",
    "    tokens= tokens.tolist()\n",
    "    reverse_vocab = {v:k for k,v in vocab.items()}\n",
    "    return \" \".join([reverse_vocab.get(token,\"UNK\") for token in tokens])\n",
    "\n",
    "def build_vocab(text, vocab_size=50000):\n",
    "    words = re.findall(r'\\b\\w+\\b',text.lower())\n",
    "    word_counts =Counter(words)\n",
    "    most_common = word_counts.most_common(vocab_size-2)\n",
    "\n",
    "    vocab ={\"<PAD>\":0, \"UNK\":1,\"<SOS>\":2,\"<EOS>\":3}\n",
    "    vocab.update({word: idx +4 for idx,(word,_) in enumerate(most_common)})\n",
    "\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a85dc5",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "066c4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "\n",
    "def preprocess_text(input_file, output_file, vocab_size:int=5000, seq_len:int=64,encoding:str='utf-8'):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding=encoding) as file:\n",
    "            text= file.read().lower()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(input_file, 'r', encoding='latin-1') as file:\n",
    "            text =file.read().lower()\n",
    "    except Exception as e:\n",
    "        print(f'Error reading file: {e}')\n",
    "    \n",
    "    vocab = build_vocab(text,vocab_size)\n",
    "    tokenize_text = tokenize(text,vocab)\n",
    "\n",
    "    data = [tokenize_text[i:i+seq_len] for i in range(0,len(tokenize_text)-seq_len,seq_len)]\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    torch.save((data, vocab), output_file)\n",
    "    print(f'Data saved to {output_file}')\n",
    "    print(f'Vocabulary size: {len(vocab)}')\n",
    "    print(f'Total token:{len(tokenize_text)}')\n",
    "    print(f'Number of seq:{len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9e036",
   "metadata": {},
   "source": [
    "# Downloading datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b566a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001-679af0bccbb2f644.parquet', 'validation': 'data/validation-00000-of-00001-089cf71e86c88e28.parquet', 'test': 'data/test-00000-of-00001-ae1348a38be3cb29.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/Sandipan1994/Inference_Text_Generation/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c098ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "step",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0f1932d5-0c49-4356-bd59-046d7dbe8c44",
       "rows": [
        [
         "0",
         "leo is a kind of constellation & a constellation contains stars"
        ],
        [
         "1",
         "leo is a constellation containing stars & the earth revolving around the sun causes stars to appear in different areas in the sky at different times of year"
        ],
        [
         "2",
         "earth is a kind of celestial object & a star is a kind of celestial object / celestial body & apparent motion is when an object appears to move relative to another object 's position"
        ],
        [
         "3",
         "apparent motion of stars is when stars appear to move relative to earth's position & the earth rotating on its axis causes stars to appear to move across the sky at night"
        ],
        [
         "4",
         "the earth rotating on its axis causes apparent motion of stars & stars appear to move relative to the horizon during the night"
        ],
        [
         "5",
         "diurnal motion is when objects in the sky appear to move due to earth 's rotation on its axis & stars appear to move relative to the horizon during the night"
        ],
        [
         "6",
         "stars apearing to move relative to the horizon during the night is an example of diurnal motion & the earth rotating on its axis causes stars / the moon to appear to move across the sky at night"
        ],
        [
         "7",
         "a star produces light & a source of something produces that something"
        ],
        [
         "8",
         "stars are a source of light & as a source of light becomes closer , the light will appear brighter"
        ],
        [
         "9",
         "as the stars become closer, the light of the stars will appear brighter & the sun is the star that is closest to earth"
        ],
        [
         "10",
         "a star produces light & a source of something produces that something"
        ],
        [
         "11",
         "stars are a source of light & as a source of light becomes closer , the light will appear brighter"
        ],
        [
         "12",
         "the sun is the star that is closest to earth & the stars in the night sky are very far away from the earth"
        ],
        [
         "13",
         "as the stars becomes closer, the light of the stars will appear brighter & the sun is closer to earth than other stars to earth"
        ],
        [
         "14",
         "more light gets reflected on highly reflective things & venus is covered in highly reflective clouds"
        ],
        [
         "15",
         "venus reflects more sunlight toward earth than other planets & as the light reflected off of an object increases , the object will appear to be brighter"
        ],
        [
         "16",
         "united states is located in the northern hemisphere & new york / new york state is a state located in the united states of america"
        ],
        [
         "17",
         "new york state is located in the northern hemisphere & june is during the summer in the northern hemisphere"
        ],
        [
         "18",
         "june is during the summer for new york state & the amount of daylight is greatest in the summer"
        ],
        [
         "19",
         "earth 's tilt on its rotating axis causes seasons to change / to occur & when the season changes , the amount of daylight will change"
        ],
        [
         "20",
         "alaska is a state located in the united states of america & united states is located in the northern hemisphere"
        ],
        [
         "21",
         "alaska is located in the northern hemisphere & alaska is in winter season & winter is when a hemisphere is tilted away from the sun"
        ],
        [
         "22",
         "when a hemisphere is tilted away from the sun , that hemisphere receives less direct sunlight & winter is when a hemisphere is tilted away from the sun"
        ],
        [
         "23",
         "the northern hemisphere is tilted away from the sun when alaska is in winter & a hemisphere tilting away from the sun causes the hemisphere to receive less direct sunlight"
        ],
        [
         "24",
         "the earth rotates on its tilted axis & earth is a kind of planet"
        ],
        [
         "25",
         "earth is a planet that rotates on its tilted axis & a planet rotating causes cycles of day and night on that planet"
        ],
        [
         "26",
         "a complete rotation of the earth on earth 's axis / itself takes 1 / one day & 1 day is equal to 24 hours"
        ],
        [
         "27",
         "new york state is a state located in the united states of america & when the season changes , the amount of daylight will change"
        ],
        [
         "28",
         "earth is a kind of planet & the earth rotates on its tilted axis"
        ],
        [
         "29",
         "earth is a planet that rotates on its tilted axis & a planet rotating causes cycles of day and night on that planet"
        ],
        [
         "30",
         "summer is when a hemisphere is tilted towards the sun & the south pole is tilted toward the sun"
        ],
        [
         "31",
         "it is summer in south pole & the south pole is located in the southern hemisphere"
        ],
        [
         "32",
         "it is summer in southern hemisphere & the winter in the northern hemisphere is during the summer in the southern hemisphere"
        ],
        [
         "33",
         "florida is a state located in the united states of america & united states is located in the northern hemisphere"
        ],
        [
         "34",
         "it is winter in the northern hemisphere & florida is located in the northern hemisphere"
        ],
        [
         "35",
         "earth is a kind of planet & the sun is a kind of star & the earth revolves around the sun"
        ],
        [
         "36",
         "earth revolving the sun is an example of a planet revolving around its star & a complete revolution / orbit of a planet around its star takes 1 / one planetary year"
        ],
        [
         "37",
         "a complete revolution of earth around the sun takes an earth year & 1 year is equal to 365 days"
        ],
        [
         "38",
         "earth is a kind of planet & the earth rotates on its tilted axis"
        ],
        [
         "39",
         "the earth is a planet that rotates on its tilted axis & a planet rotating causes cycles of day and night on that planet"
        ],
        [
         "40",
         "the earth rotating on its tilted axis causes the cycles of day and night on earth & the cycles of day and night is when the sun rises during the day and the sun sets at night"
        ],
        [
         "41",
         "the earth rotates on its tilted axis & earth is a kind of planet"
        ],
        [
         "42",
         "the earth is a planet that rotates on its tilted axis & a planet rotating causes cycles of day and night on that planet"
        ],
        [
         "43",
         "earth is a kind of planet & the earth rotates on its tilted axis"
        ],
        [
         "44",
         "the earth is a planet that rotates on its tilted axis & a planet rotating causes cycles of day and night on that planet"
        ],
        [
         "45",
         "earth is a kind of planet & the sun is a kind of star & the earth revolves around the sun"
        ],
        [
         "46",
         "earth revolving the sun is an example of a planet revolving around its star & a complete revolution / orbit of a planet around its star takes 1 / one planetary year"
        ],
        [
         "47",
         "the earth revolves around the sun & the sun is a kind of star & earth is a kind of planet"
        ],
        [
         "48",
         "earth revolving the sun is an example of a planet revolving around its star & a complete revolution / orbit of a planet around its star takes 1 / one planetary year"
        ],
        [
         "49",
         "the sun setting is a kind of event & the sun rising is a kind of event & the sun rising / setting occurs once per day"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5881
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leo is a kind of constellation &amp; a constellati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leo is a constellation containing stars &amp; the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earth is a kind of celestial object &amp; a star i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apparent motion of stars is when stars appear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the earth rotating on its axis causes apparent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>as heat is transferred from something to somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>the temperature of the liquid will decrease &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>the heat energy of the liquid will decrease &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>the temperature of the liquid will decrease &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>scientists go to a lake once a month to take s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5881 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   step\n",
       "0     leo is a kind of constellation & a constellati...\n",
       "1     leo is a constellation containing stars & the ...\n",
       "2     earth is a kind of celestial object & a star i...\n",
       "3     apparent motion of stars is when stars appear ...\n",
       "4     the earth rotating on its axis causes apparent...\n",
       "...                                                 ...\n",
       "5876  as heat is transferred from something to somet...\n",
       "5877  the temperature of the liquid will decrease & ...\n",
       "5878  the heat energy of the liquid will decrease & ...\n",
       "5879  the temperature of the liquid will decrease & ...\n",
       "5880  scientists go to a lake once a month to take s...\n",
       "\n",
       "[5881 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = pd.read_parquet(\"hf://datasets/Sandipan1994/Inference_Text_Generation/\" + splits[\"validation\"])\n",
    "df_text = pd.read_parquet(\"hf://datasets/Sandipan1994/Inference_Text_Generation/\" + splits[\"test\"])\n",
    "df_d = pd.concat([df,df_val,df_text],ignore_index=True)\n",
    "df_d.drop(columns=['inference'],inplace=True)\n",
    "df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c94250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to E:/data sciences/LLMs/gen/text_generation/data.csv\n"
     ]
    }
   ],
   "source": [
    "path ='E:/data sciences/LLMs/gen/text_generation/data.csv'\n",
    "df_d.to_csv(path,index=False)\n",
    "print(f'Saved to {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26579423",
   "metadata": {},
   "source": [
    "# convert CSV to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "031cb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "text_file='E:/data sciences/LLMs/gen/text_generation/data.txt'\n",
    "with open(text_file,\"w\") as output_file:\n",
    "    with open(path,\"r\") as input_file:\n",
    "        [output_file.write(\" \".join(row)+'\\n') for row in csv.reader(input_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9d8d6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to E:/data sciences/LLMs/gen/text_generation/token.pt\n",
      "Vocabulary size: 4079\n",
      "Total token:121076\n",
      "Number of seq:1891\n"
     ]
    }
   ],
   "source": [
    "data_tokens ='E:/data sciences/LLMs/gen/text_generation/token.pt'\n",
    "preprocess_text(text_file,data_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e870589",
   "metadata": {},
   "source": [
    "# Building a data pipeline for feeding data in batches to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2f85ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_tokens,'rb') as file:\n",
    "    data, vocab = torch.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa526301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,text_data,tokenizer,voc, max_len):\n",
    "        self.text_data = text_data \n",
    "        self.tokenizer= tokenizer\n",
    "        self.voc= voc\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data) # number of samples in the dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data= self.text_data[index]\n",
    "\n",
    "        # tokenizer the text data\n",
    "        data_token = self.tokenizer(data, self.voc)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(data_token) >self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens= data_token\n",
    "        \n",
    "        # create input tensor (padded)\n",
    "        input_ids = torch.zeros(self.max_len,dtype=torch.long)\n",
    "        input_ids[:len(tokens)] = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "        # create target_ids = inputs_ids shifted left twice plus for <SOS>\n",
    "        target_ids = torch.zeros(self.max_len, dtype=torch.long)\n",
    "        if len(tokens)>1:\n",
    "            target_ids[:len(tokens)-2] =torch.tensor(tokens[2:], dtype=torch.long) \n",
    "\n",
    "        return input_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6994402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids:  tensor([   2, 2469,    6,    4, 2099,    3,    0,    0,    0,    0])\n",
      "output ids:  tensor([   6,    4, 2099,    3,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "text_data= [\n",
    "    \"leo is a constellation\",\n",
    "    \"earth rotates on its axis\"\n",
    "]\n",
    "see = CustomTextDataset(text_data, tokenize, vocab, max_len=10)\n",
    "inp, tgt = see[0]\n",
    "print(\"input ids: \", inp)\n",
    "print(\"output ids: \", tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd91630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfedc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e97aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78aaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa244500",
   "metadata": {},
   "source": [
    "# Model Architacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41c8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model, num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert d_model % num_heads ==0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # initialize dimensions\n",
    "        self.d_model = d_model\n",
    "        self.num_heads= num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # linear layers for transforming the inputs queries, keys and valuea\n",
    "        self.W_q = nn.Linear(d_model,d_model)\n",
    "        self.W_k = nn.Linear(d_model,d_model)\n",
    "        self.W_v = nn.Linear(d_model,d_model)\n",
    "        self.W_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def scaled_dot_product_aatention(self,q,k,v,mask=None):\n",
    "        attn_scores =torch.matmul(q,k.transpose(-2,-1))/ math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores =attn_scores.masked_fill(mask==0,float('-inf'))\n",
    "        attn_probs = torch.softmax(attn_scores,dim=-1)\n",
    "\n",
    "        output = torch.matmul(attn_scores,v)\n",
    "        return output\n",
    "    def split_head(self,x):\n",
    "        # reshape the input tensor to (batch_size, seq_len, num_heads, d_k)\n",
    "        batch_size, seq_len, d_model =x.size()\n",
    "        return x.view(batch_size,seq_len, self.num_heads,self.d_k).transpose(1,2)\n",
    "    \n",
    "    def combine_heads(self,x):\n",
    "        batch_size, num_heads, seq_len, d_k =x.size()\n",
    "        return x.transpose(1,2).contiguous().view()(batch_size,seq_len,self.d_model)\n",
    "    \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        # linear transformation and split\n",
    "        Q= self.split_head(self.W_q(q))\n",
    "        K = self.split_head(self.W_k(k))\n",
    "        V = self.split_head(self.W_v(v))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_aatention(Q,K,V,mask)\n",
    "        attn_output =self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0620aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super(PositionWiseFeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        self.activation =nn.GeLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8978d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_len):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,d_model)\n",
    "        position =torch.arange(0, max_seq_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float()* - (math.log(10000.0)/d_model))\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414516de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads,d_ff,dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 =nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        attn_output = self.self_attn(x,x,x,mask)\n",
    "        x =self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x+self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e933b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size, d_model, num_heads, num_layers, d_ff,max_seq_len, dropout,device):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.positional_encoding= PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc= nn.Linear(d_model,vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def genarate_mask (self,data):\n",
    "        # padding mask\n",
    "        pad_mask = (data !=0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # casual mask (seq_len, seq_len)\n",
    "        seq_len = data.size(1)\n",
    "        casual_mask = torch.tril(torch.ones((seq_len,seq_len),device=self.device)).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        # combine all only non-pad + past tokens\n",
    "        mask = pad_mask & casual_mask\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, data):\n",
    "        mask =self.genarate_mask(data)\n",
    "\n",
    "        data_embed = self.dropout(self.positional_encoding(self.decoder_embedding(data)))\n",
    "        x = data_embed\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x,mask)\n",
    "        output = self.fc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd8046",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff2f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config['vocab_size'],\n",
    "                    config['d_model'],\n",
    "                    config['num_heads'],\n",
    "                    config['num_layers'],\n",
    "                    config['d_ff'],\n",
    "                    config['vocab_size'],\n",
    "                    config['dropout'],\n",
    "                    device=device\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(vocab_size, config, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "\n",
    "    # Create checkpoint folder\n",
    "    os.makedirs(os.path.dirname(config['checkpoint_path']), exist_ok=True)\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_seq, target_seq in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_seq)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "            # Flatten for CE Loss\n",
    "            loss = criterion(\n",
    "                logits.view(-1, vocab_size),\n",
    "                target_seq.view(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * input_seq.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), config['checkpoint_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
